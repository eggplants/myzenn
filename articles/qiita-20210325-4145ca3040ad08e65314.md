---
title: "任意サイトのサブドメインを列挙する"
emoji: "😀"
type: "tech"
topics: [Python,Bash,curl,シェル芸]
published: true
---
## 何？

あるサイトのサブドメインを探して列挙したい。
例えば`example.com`なら`ftp.example.com`や`ws3.example.com`などがサブドメインに当たる。
調べたところ[いくつかツール](https://github.com/search?o=desc&q=subdomain+enumeration&s=stars&type=Repositories)がヒットした。その中でも`sublist3r`がシンプルで良かったので紹介する。

## `sublist3r`

[aboul3la/Sublist3r](https://github.com/aboul3la/Sublist3r)は、各検索エンジンのクローラが拾ったレコードからサブドメインを検索し列挙するコマンド。(メンテされてるか微妙だが2020-03-25現在使用できた。)

インストールは:

```bash:Terminal
# pypiには第三者がアップロードしているので本家repoから
$ git clone https://github.com/aboul3la/Sublist3r && cd Sublist3r
$ pip install -r requirements.txt
$ pip install -e .
$ sublist3r --help
```

```text:Help
usage: sublist3r [-h] -d DOMAIN [-b [BRUTEFORCE]] [-p PORTS] [-v [VERBOSE]] [-t THREADS]
                 [-e ENGINES] [-o OUTPUT] [-n]

OPTIONS:
  -h, --help            show this help message and exit
  -d DOMAIN, --domain DOMAIN
                        Domain name to enumerate it's subdomains
  -b [BRUTEFORCE], --bruteforce [BRUTEFORCE]
                        Enable the subbrute bruteforce module
  -p PORTS, --ports PORTS
                        Scan the found subdomains against specified tcp ports
  -v [VERBOSE], --verbose [VERBOSE]
                        Enable Verbosity and display results in realtime
  -t THREADS, --threads THREADS
                        Number of threads to use for subbrute bruteforce
  -e ENGINES, --engines ENGINES
                        Specify a comma-separated list of search engines
  -o OUTPUT, --output OUTPUT
                        Save the results to text file
  -n, --no-color        Output without color

Example: python /home/eggplants/.local/bin/sublist3r -d google.com
```

## 実際に列挙

`sublist3r -d <domain>`で実行できる。プレーンテキストで保存するには`-o <filename>`を使う。

今回は筑波大学のドメイン`tsukuba.ac.jp`で試してみる。

```bash:Terminal
$ LANG=C date
Thu Mar 25 11:25:01 JST 2021 
$ sublist3r -d tsukuba.ac.jp -o subdomains.txt
...(略)...
$ wc -l subdomains.txt
13876
```

結果は[こちら](https://gist.github.com/eggplants/a66fffa1bee3db8fb50d82319b4040f5)

## 応用

```bash:Terminal
# 自己責任で
$ (echo 'url,res_code,content_type,res_time'
  cat subdomains.txt | xargs -n 1 -P 5 -I@ \
  curl "@" -m 2 -s -o /dev/null \
       -w "@,%{http_code},%{content_type},%{time_starttransfer}s\n" | sort) > res.csv
# httpでなにか配信されているものの列挙
$ (sed 1\!d res.csv;grep -v ,, res.csv | grep ,200,) > res_200.csv
$ echo $[`wc -l<res_200.csv`-1]
279
# httpsとかへの転送(301)
$ (sed 1\!d res.csv;grep -v ,, res.csv | grep ,301,) > res_301.csv
$ echo $[`wc -l<res_301.csv`-1]
180
```

結果は[こちら](https://gist.github.com/eggplants/b42c4136243317e27c2cdc0d04d78755)
またこれらの処理をまとめた[スクリプト](https://github.com/eggplants/sres)を書いた。

## 結論

お手軽。

